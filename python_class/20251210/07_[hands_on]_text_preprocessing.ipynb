{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "4c5e02e1",
      "metadata": {
        "id": "4c5e02e1"
      },
      "source": [
        "# [Hands-On] 텍스트 전처리 파이프라인 구축\n",
        "\n",
        "- Author: Sangkeun Jung (hugmanskj@gmail.com)\n",
        "\n",
        "> 교육 목적\n",
        "\n",
        "**Copyright**: All rights reserved\n",
        "\n",
        "---\n",
        "\n",
        "## 개요\n",
        "\n",
        "텍스트 데이터의 전처리 파이프라인을 구축하는 실습입니다.\n",
        "불용어 제거, 표준화, 정규화 기법을 학습하고 의료 판독문에 적용합니다.\n",
        "\n",
        "\n",
        "**학습 목표**:\n",
        "1. 불용어(Stopwords) 제거 방법 이해 및 구현\n",
        "2. 표준화(Normalization) 기법 습득\n",
        "3. 정규화(Regularization) 기법 습득\n",
        "4. 전처리 파이프라인 통합 구축"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6c157cc3",
      "metadata": {
        "id": "6c157cc3"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b577e3f0",
      "metadata": {
        "id": "b577e3f0"
      },
      "source": [
        "## 1. 라이브러리 임포트"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "851939fb",
      "metadata": {
        "id": "851939fb"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import numpy as np\n",
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aad7e374",
      "metadata": {
        "id": "aad7e374"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e0972d6a",
      "metadata": {
        "id": "e0972d6a"
      },
      "source": [
        "## 2. 불용어(Stopwords) 제거\n",
        "\n",
        "### 불용어란?\n",
        "- 텍스트 분석에서 의미가 없거나 중요하지 않은 단어\n",
        "- 한국어: 조사, 어미, 접속사 등\n",
        "- 영어: a, an, the, is, are 등\n",
        "\n",
        "### 불용어 제거의 효과\n",
        "- Vocabulary 크기 감소 (30-50% 감소)\n",
        "- 계산 효율성 증가\n",
        "- 의미 있는 단어에 집중"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e9da8732",
      "metadata": {
        "id": "e9da8732"
      },
      "outputs": [],
      "source": [
        "# 한국어 불용어 리스트 (예시)\n",
        "KOREAN_STOPWORDS = set([\n",
        "    # 조사\n",
        "    '이', '가', '을', '를', '은', '는', '에', '에서', '으로', '로',\n",
        "    '의', '와', '과', '도', '만', '부터', '까지', '께서', '에게', '한테',\n",
        "\n",
        "    # 어미\n",
        "    '다', '이다', '입니다', '습니다', '아', '어', '었', '였', '네', '요',\n",
        "\n",
        "    # 접속사\n",
        "    '그리고', '하지만', '그러나', '그래서', '또한', '또는', '및',\n",
        "\n",
        "    # 부사\n",
        "    '매우', '아주', '너무', '정말', '진짜', '완전', '조금', '약간', '많이',\n",
        "\n",
        "    # 대명사\n",
        "    '이것', '그것', '저것', '여기', '거기', '저기', '이곳', '그곳', '저곳',\n",
        "\n",
        "    # 기타\n",
        "    '것', '수', '등', '때', '중', '내', '간'\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "caa15e7c",
      "metadata": {
        "lines_to_next_cell": 1,
        "id": "caa15e7c"
      },
      "outputs": [],
      "source": [
        "# 영어 불용어 리스트 (예시)\n",
        "ENGLISH_STOPWORDS = set([\n",
        "    'a', 'an', 'the', 'and', 'or', 'but', 'if', 'then', 'else',\n",
        "    'is', 'are', 'was', 'were', 'be', 'been', 'being',\n",
        "    'have', 'has', 'had', 'do', 'does', 'did',\n",
        "    'i', 'you', 'he', 'she', 'it', 'we', 'they',\n",
        "    'my', 'your', 'his', 'her', 'its', 'our', 'their',\n",
        "    'this', 'that', 'these', 'those',\n",
        "    'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by', 'from',\n",
        "    'very', 'too', 'so', 'much', 'many', 'more', 'most'\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f70a5f96",
      "metadata": {
        "id": "f70a5f96"
      },
      "source": [
        "### 불용어 제거 함수 구현"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fdfc331d",
      "metadata": {
        "lines_to_next_cell": 1,
        "id": "fdfc331d"
      },
      "outputs": [],
      "source": [
        "def remove_stopwords(tokens, stopwords):\n",
        "    \"\"\"\n",
        "    토큰 리스트에서 불용어를 제거하는 함수\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    tokens : list of str\n",
        "        토큰 리스트\n",
        "    stopwords : set of str\n",
        "        불용어 집합\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    list of str\n",
        "        불용어가 제거된 토큰 리스트\n",
        "    \"\"\"\n",
        "    filtered_tokens = [token for token in tokens if token not in stopwords]\n",
        "    return filtered_tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4f184ab7",
      "metadata": {
        "id": "4f184ab7"
      },
      "source": [
        "### 예제 1: 한국어 불용어 제거"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1d7d1c89",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1d7d1c89",
        "outputId": "05956714-102f-4fdb-df2f-9c16a9b281ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "[예제 1] 한국어 불용어 제거\n",
            "================================================================================\n",
            "\n",
            "원본 문장:\n",
            "  \"나는 오늘 아주 매우 많이 피곤했다\"\n",
            "\n",
            "토큰 리스트:\n",
            "  ['나는', '오늘', '아주', '매우', '많이', '피곤했다']\n",
            "\n",
            "불용어 제거 후:\n",
            "  ['나는', '오늘', '피곤했다']\n",
            "\n",
            "제거된 불용어:\n",
            "  ['아주', '매우', '많이']\n",
            "\n",
            "감소율: 50.0% (6 → 3 토큰)\n"
          ]
        }
      ],
      "source": [
        "print(\"=\" * 80)\n",
        "print(\"[예제 1] 한국어 불용어 제거\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# 예제 문장\n",
        "sentence1 = \"나는 오늘 아주 매우 많이 피곤했다\"\n",
        "tokens1 = sentence1.split()\n",
        "\n",
        "print(\"\\n원본 문장:\")\n",
        "print(\"  \\\"%s\\\"\" % sentence1)\n",
        "print(\"\\n토큰 리스트:\")\n",
        "print(\"  %s\" % tokens1)\n",
        "\n",
        "# 불용어 제거\n",
        "filtered1 = remove_stopwords(tokens1, KOREAN_STOPWORDS)\n",
        "\n",
        "print(\"\\n불용어 제거 후:\")\n",
        "print(\"  %s\" % filtered1)\n",
        "print(\"\\n제거된 불용어:\")\n",
        "print(\"  %s\" % [t for t in tokens1 if t not in filtered1])\n",
        "print(\"\\n감소율: %.1f%% (%d → %d 토큰)\" % (\n",
        "    100.0 * (len(tokens1) - len(filtered1)) / len(tokens1),\n",
        "    len(tokens1),\n",
        "    len(filtered1)\n",
        "))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f9c18382",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f9c18382",
        "outputId": "5ab9780e-9a89-4adf-8e35-039381fc2d5c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "원본 문장:\n",
            "  \"비가 많이 내렸다 그래서 나는 우산을 가지고 나갔다\"\n",
            "\n",
            "토큰 리스트:\n",
            "  ['비가', '많이', '내렸다', '그래서', '나는', '우산을', '가지고', '나갔다']\n",
            "\n",
            "불용어 제거 후:\n",
            "  ['비가', '내렸다', '나는', '우산을', '가지고', '나갔다']\n",
            "\n",
            "제거된 불용어:\n",
            "  ['많이', '그래서']\n",
            "\n",
            "감소율: 25.0% (8 → 6 토큰)\n"
          ]
        }
      ],
      "source": [
        "# 예제 문장 2\n",
        "sentence2 = \"비가 많이 내렸다 그래서 나는 우산을 가지고 나갔다\"\n",
        "tokens2 = sentence2.split()\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"원본 문장:\")\n",
        "print(\"  \\\"%s\\\"\" % sentence2)\n",
        "print(\"\\n토큰 리스트:\")\n",
        "print(\"  %s\" % tokens2)\n",
        "\n",
        "filtered2 = remove_stopwords(tokens2, KOREAN_STOPWORDS)\n",
        "\n",
        "print(\"\\n불용어 제거 후:\")\n",
        "print(\"  %s\" % filtered2)\n",
        "print(\"\\n제거된 불용어:\")\n",
        "print(\"  %s\" % [t for t in tokens2 if t not in filtered2])\n",
        "print(\"\\n감소율: %.1f%% (%d → %d 토큰)\" % (\n",
        "    100.0 * (len(tokens2) - len(filtered2)) / len(tokens2),\n",
        "    len(tokens2),\n",
        "    len(filtered2)\n",
        "))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0e49fc68",
      "metadata": {
        "id": "0e49fc68"
      },
      "source": [
        "### 예제 2: 영어 불용어 제거"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a15c7189",
      "metadata": {
        "lines_to_next_cell": 1,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a15c7189",
        "outputId": "5261cb48-698d-4226-947b-852a709d189e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "[예제 2] 영어 불용어 제거\n",
            "================================================================================\n",
            "\n",
            "원본 문장:\n",
            "  \"The patient has a mild cough and fever but no chest pain\"\n",
            "\n",
            "토큰 리스트 (소문자):\n",
            "  ['the', 'patient', 'has', 'a', 'mild', 'cough', 'and', 'fever', 'but', 'no', 'chest', 'pain']\n",
            "\n",
            "불용어 제거 후:\n",
            "  ['patient', 'mild', 'cough', 'fever', 'no', 'chest', 'pain']\n",
            "\n",
            "제거된 불용어:\n",
            "  ['the', 'has', 'a', 'and', 'but']\n",
            "\n",
            "감소율: 41.7% (12 → 7 토큰)\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"[예제 2] 영어 불용어 제거\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "sentence_en = \"The patient has a mild cough and fever but no chest pain\"\n",
        "tokens_en = sentence_en.lower().split()\n",
        "\n",
        "print(\"\\n원본 문장:\")\n",
        "print(\"  \\\"%s\\\"\" % sentence_en)\n",
        "print(\"\\n토큰 리스트 (소문자):\")\n",
        "print(\"  %s\" % tokens_en)\n",
        "\n",
        "filtered_en = remove_stopwords(tokens_en, ENGLISH_STOPWORDS)\n",
        "\n",
        "print(\"\\n불용어 제거 후:\")\n",
        "print(\"  %s\" % filtered_en)\n",
        "print(\"\\n제거된 불용어:\")\n",
        "print(\"  %s\" % [t for t in tokens_en if t not in filtered_en])\n",
        "print(\"\\n감소율: %.1f%% (%d → %d 토큰)\" % (\n",
        "    100.0 * (len(tokens_en) - len(filtered_en)) / len(tokens_en),\n",
        "    len(tokens_en),\n",
        "    len(filtered_en)\n",
        "))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ea93086c",
      "metadata": {
        "id": "ea93086c"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1be7aa7a",
      "metadata": {
        "id": "1be7aa7a"
      },
      "source": [
        "## 3. 표준화(Normalization)\n",
        "\n",
        "### 표준화란?\n",
        "- 동일한 의미를 가진 다양한 형태의 텍스트를 통일된 형태로 변환\n",
        "- 예: \"좋아요\", \"좋아아아요\", \"좋아아아아아요\" → \"좋아요\"\n",
        "\n",
        "### 표준화 기법\n",
        "1. 대소문자 통일 (영어)\n",
        "2. 반복 문자 축약\n",
        "3. 특수문자 처리\n",
        "4. 공백 정규화"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ca774e3b",
      "metadata": {
        "id": "ca774e3b"
      },
      "source": [
        "### 표준화 함수 구현"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0549780e",
      "metadata": {
        "lines_to_next_cell": 1,
        "id": "0549780e"
      },
      "outputs": [],
      "source": [
        "def normalize_text(text):\n",
        "    \"\"\"\n",
        "    텍스트를 표준화하는 함수\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    text : str\n",
        "        원본 텍스트\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    str\n",
        "        표준화된 텍스트\n",
        "    \"\"\"\n",
        "    # 1. 소문자 변환 (영어)\n",
        "    text = text.lower()\n",
        "\n",
        "    # 2. 반복되는 문자 축약 (3개 이상 → 1개)\n",
        "    # 예: \"좋아아아아요\" → \"좋아요\"\n",
        "    text = re.sub(r'(.)\\1{2,}', r'\\1', text)\n",
        "\n",
        "    # 3. 반복되는 구두점 축약 (2개 이상 → 1개)\n",
        "    # 예: \"!!!!\" → \"!\"\n",
        "    text = re.sub(r'([!?.])\\1+', r'\\1', text)\n",
        "\n",
        "    # 4. 다중 공백을 단일 공백으로\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "\n",
        "    # 5. 앞뒤 공백 제거\n",
        "    text = text.strip()\n",
        "\n",
        "    return text"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4ed3a64e",
      "metadata": {
        "id": "4ed3a64e"
      },
      "source": [
        "### 예제 3: 표준화 적용"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "63075a36",
      "metadata": {
        "lines_to_next_cell": 1,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "63075a36",
        "outputId": "42ee7f19-54cc-4f3c-bc27-f4582ed9a915"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "[예제 3] 텍스트 표준화\n",
            "================================================================================\n",
            "\n",
            "[1] 원본:\n",
            "    \"좋아아아아요!!! 정말 최고예요!!!\"\n",
            "    표준화:\n",
            "    \"좋아요! 정말 최고예요!\"\n",
            "    길이: 20 → 13 (35.0% 감소)\n",
            "\n",
            "[2] 원본:\n",
            "    \"ㅋㅋㅋㅋㅋㅋㅋㅋ 진짜    너무   웃겨요  ㅎㅎㅎ\"\n",
            "    표준화:\n",
            "    \"ㅋ 진짜 너무 웃겨요 ㅎ\"\n",
            "    길이: 28 → 13 (53.6% 감소)\n",
            "\n",
            "[3] 원본:\n",
            "    \"HELLO World!!!   HOW    ARE   YOU???\"\n",
            "    표준화:\n",
            "    \"hello world! how are you?\"\n",
            "    길이: 36 → 25 (30.6% 감소)\n",
            "\n",
            "[4] 원본:\n",
            "    \"대박!!!!!! 완전 신기해요오오오오\"\n",
            "    표준화:\n",
            "    \"대박! 완전 신기해요오\"\n",
            "    길이: 20 → 12 (40.0% 감소)\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"[예제 3] 텍스트 표준화\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# 예제 텍스트들\n",
        "test_texts = [\n",
        "    \"좋아아아아요!!! 정말 최고예요!!!\",\n",
        "    \"ㅋㅋㅋㅋㅋㅋㅋㅋ 진짜    너무   웃겨요  ㅎㅎㅎ\",\n",
        "    \"HELLO World!!!   HOW    ARE   YOU???\",\n",
        "    \"대박!!!!!! 완전 신기해요오오오오\"\n",
        "]\n",
        "\n",
        "for i, text in enumerate(test_texts, 1):\n",
        "    normalized = normalize_text(text)\n",
        "    print(\"\\n[%d] 원본:\" % i)\n",
        "    print(\"    \\\"%s\\\"\" % text)\n",
        "    print(\"    표준화:\")\n",
        "    print(\"    \\\"%s\\\"\" % normalized)\n",
        "    print(\"    길이: %d → %d (%.1f%% 감소)\" % (\n",
        "        len(text),\n",
        "        len(normalized),\n",
        "        100.0 * (len(text) - len(normalized)) / len(text)\n",
        "    ))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "37c66d4f",
      "metadata": {
        "id": "37c66d4f"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "829aad6c",
      "metadata": {
        "id": "829aad6c"
      },
      "source": [
        "## 4. 정규화(Regularization)\n",
        "\n",
        "### 정규화란?\n",
        "- 특정 패턴을 가진 텍스트를 대표 토큰으로 치환\n",
        "- 예: \"2024년 12월 1일\" → \"DATE\"\n",
        "- 예: \"010-1234-5678\" → \"PHONE\"\n",
        "- 예: \"12,000원\" → \"NUM원\"\n",
        "\n",
        "### 정규화의 장점\n",
        "- Vocabulary 크기 대폭 감소\n",
        "- 일반화 성능 향상\n",
        "- 숫자/날짜/전화번호 등의 의미 보존"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "03c0ffd0",
      "metadata": {
        "id": "03c0ffd0"
      },
      "source": [
        "### 정규화 함수 구현"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "96562c7f",
      "metadata": {
        "lines_to_next_cell": 1,
        "id": "96562c7f"
      },
      "outputs": [],
      "source": [
        "def normalize_numbers(text):\n",
        "    \"\"\"\n",
        "    숫자를 NUM 토큰으로 치환\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    text : str\n",
        "        원본 텍스트\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    str\n",
        "        숫자가 정규화된 텍스트\n",
        "    \"\"\"\n",
        "    # 천 단위 구분 쉼표가 있는 숫자 (예: 12,000)\n",
        "    text = re.sub(r'\\d{1,3}(,\\d{3})+', 'NUM', text)\n",
        "\n",
        "    # 소수점 숫자 (예: 3.14, 98.6)\n",
        "    text = re.sub(r'\\d+\\.\\d+', 'NUM', text)\n",
        "\n",
        "    # 일반 정수 (예: 123, 45)\n",
        "    text = re.sub(r'\\d+', 'NUM', text)\n",
        "\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e19a3b55",
      "metadata": {
        "lines_to_next_cell": 1,
        "id": "e19a3b55"
      },
      "outputs": [],
      "source": [
        "def normalize_dates(text):\n",
        "    \"\"\"\n",
        "    날짜를 DATE 토큰으로 치환\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    text : str\n",
        "        원본 텍스트\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    str\n",
        "        날짜가 정규화된 텍스트\n",
        "    \"\"\"\n",
        "    # 패턴 1: YYYY-MM-DD, YYYY/MM/DD, YYYY.MM.DD\n",
        "    text = re.sub(r'\\d{4}[-/.]\\d{1,2}[-/.]\\d{1,2}', 'DATE', text)\n",
        "\n",
        "    # 패턴 2: YYYY년MM월DD일\n",
        "    text = re.sub(r'\\d{4}년\\s?\\d{1,2}월\\s?\\d{1,2}일', 'DATE', text)\n",
        "\n",
        "    # 패턴 3: MM/DD/YYYY (미국식)\n",
        "    text = re.sub(r'\\d{1,2}/\\d{1,2}/\\d{4}', 'DATE', text)\n",
        "\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd098db3",
      "metadata": {
        "lines_to_next_cell": 1,
        "id": "fd098db3"
      },
      "outputs": [],
      "source": [
        "def normalize_phone_numbers(text):\n",
        "    \"\"\"\n",
        "    전화번호를 PHONE 토큰으로 치환\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    text : str\n",
        "        원본 텍스트\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    str\n",
        "        전화번호가 정규화된 텍스트\n",
        "    \"\"\"\n",
        "    # 패턴 1: XXX-XXXX-XXXX\n",
        "    text = re.sub(r'\\d{2,3}-\\d{3,4}-\\d{4}', 'PHONE', text)\n",
        "\n",
        "    # 패턴 2: XXXXXXXXXXX (연속된 숫자)\n",
        "    text = re.sub(r'\\d{10,11}', 'PHONE', text)\n",
        "\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fe84322a",
      "metadata": {
        "lines_to_next_cell": 1,
        "id": "fe84322a"
      },
      "outputs": [],
      "source": [
        "def normalize_patterns(text):\n",
        "    \"\"\"\n",
        "    모든 패턴을 정규화하는 통합 함수\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    text : str\n",
        "        원본 텍스트\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    str\n",
        "        패턴이 정규화된 텍스트\n",
        "    \"\"\"\n",
        "    # 순서 중요: 전화번호 → 날짜 → 숫자\n",
        "    text = normalize_phone_numbers(text)\n",
        "    text = normalize_dates(text)\n",
        "    text = normalize_numbers(text)\n",
        "\n",
        "    return text"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7f6d7b94",
      "metadata": {
        "id": "7f6d7b94"
      },
      "source": [
        "### 예제 4: 정규화 적용"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "73b4b727",
      "metadata": {
        "lines_to_next_cell": 1,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "73b4b727",
        "outputId": "28cfb3b8-d0e7-4ad7-9988-c5abaac7576c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "[예제 4] 패턴 정규화\n",
            "================================================================================\n",
            "\n",
            "[1] 원본:\n",
            "    \"2024년12월1일에 010-1234-5678로 연락주세요.\"\n",
            "    정규화:\n",
            "    \"DATE에 PHONE로 연락주세요.\"\n",
            "\n",
            "[2] 원본:\n",
            "    \"가격은 12,000원이고 무게는 3.5kg입니다.\"\n",
            "    정규화:\n",
            "    \"가격은 NUM원이고 무게는 NUMkg입니다.\"\n",
            "\n",
            "[3] 원본:\n",
            "    \"예약 날짜: 2024-12-15, 전화: 02-1234-5678\"\n",
            "    정규화:\n",
            "    \"예약 날짜: DATE, 전화: PHONE\"\n",
            "\n",
            "[4] 원본:\n",
            "    \"검사 비용: 150,000원 (2024.12.01 기준)\"\n",
            "    정규화:\n",
            "    \"검사 비용: NUM원 (DATE 기준)\"\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"[예제 4] 패턴 정규화\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "test_cases = [\n",
        "    \"2024년12월1일에 010-1234-5678로 연락주세요.\",\n",
        "    \"가격은 12,000원이고 무게는 3.5kg입니다.\",\n",
        "    \"예약 날짜: 2024-12-15, 전화: 02-1234-5678\",\n",
        "    \"검사 비용: 150,000원 (2024.12.01 기준)\"\n",
        "]\n",
        "\n",
        "for i, text in enumerate(test_cases, 1):\n",
        "    normalized = normalize_patterns(text)\n",
        "    print(\"\\n[%d] 원본:\" % i)\n",
        "    print(\"    \\\"%s\\\"\" % text)\n",
        "    print(\"    정규화:\")\n",
        "    print(\"    \\\"%s\\\"\" % normalized)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "05b8d290",
      "metadata": {
        "id": "05b8d290"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5a185799",
      "metadata": {
        "id": "5a185799"
      },
      "source": [
        "## 5. 통합 전처리 파이프라인\n",
        "\n",
        "### TextPreprocessor 클래스\n",
        "- 모든 전처리 단계를 하나의 파이프라인으로 통합\n",
        "- 각 단계를 선택적으로 적용 가능"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a861912a",
      "metadata": {
        "lines_to_next_cell": 1,
        "id": "a861912a"
      },
      "outputs": [],
      "source": [
        "class TextPreprocessor:\n",
        "    \"\"\"\n",
        "    텍스트 전처리 파이프라인 클래스\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 stopwords=None,\n",
        "                 remove_stopwords_flag=True,\n",
        "                 normalize_flag=True,\n",
        "                 regularize_flag=True):\n",
        "        \"\"\"\n",
        "        Parameters:\n",
        "        -----------\n",
        "        stopwords : set of str\n",
        "            불용어 집합\n",
        "        remove_stopwords_flag : bool\n",
        "            불용어 제거 여부\n",
        "        normalize_flag : bool\n",
        "            표준화 적용 여부\n",
        "        regularize_flag : bool\n",
        "            정규화 적용 여부\n",
        "        \"\"\"\n",
        "        self.stopwords = stopwords if stopwords is not None else set()\n",
        "        self.remove_stopwords_flag = remove_stopwords_flag\n",
        "        self.normalize_flag = normalize_flag\n",
        "        self.regularize_flag = regularize_flag\n",
        "\n",
        "    def preprocess(self, text):\n",
        "        \"\"\"\n",
        "        텍스트를 전처리하는 메인 함수\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        text : str\n",
        "            원본 텍스트\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        list of str\n",
        "            전처리된 토큰 리스트\n",
        "        \"\"\"\n",
        "        # 1. 정규화 (패턴 치환)\n",
        "        if self.regularize_flag:\n",
        "            text = normalize_patterns(text)\n",
        "\n",
        "        # 2. 표준화 (대소문자, 반복 문자 등)\n",
        "        if self.normalize_flag:\n",
        "            text = normalize_text(text)\n",
        "\n",
        "        # 3. 토큰화 (공백 기준)\n",
        "        tokens = text.split()\n",
        "\n",
        "        # 4. 불용어 제거\n",
        "        if self.remove_stopwords_flag:\n",
        "            tokens = remove_stopwords(tokens, self.stopwords)\n",
        "\n",
        "        return tokens\n",
        "\n",
        "    def preprocess_full(self, text):\n",
        "        \"\"\"\n",
        "        전처리 전후 비교를 위한 함수\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        dict\n",
        "            원본 텍스트, 각 단계별 결과, 최종 결과\n",
        "        \"\"\"\n",
        "        results = {\n",
        "            'original': text,\n",
        "            'regularized': text,\n",
        "            'normalized': text,\n",
        "            'tokenized': [],\n",
        "            'filtered': []\n",
        "        }\n",
        "\n",
        "        # 1. 정규화\n",
        "        if self.regularize_flag:\n",
        "            results['regularized'] = normalize_patterns(text)\n",
        "\n",
        "        # 2. 표준화\n",
        "        if self.normalize_flag:\n",
        "            results['normalized'] = normalize_text(results['regularized'])\n",
        "\n",
        "        # 3. 토큰화\n",
        "        results['tokenized'] = results['normalized'].split()\n",
        "\n",
        "        # 4. 불용어 제거\n",
        "        if self.remove_stopwords_flag:\n",
        "            results['filtered'] = remove_stopwords(\n",
        "                results['tokenized'],\n",
        "                self.stopwords\n",
        "            )\n",
        "        else:\n",
        "            results['filtered'] = results['tokenized']\n",
        "\n",
        "        return results"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "48bd0358",
      "metadata": {
        "id": "48bd0358"
      },
      "source": [
        "### 예제 5: TextPreprocessor 사용"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "861fa0cc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "861fa0cc",
        "outputId": "7d94a169-61b5-43a7-c831-fdb7b1564182"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "[예제 5] 통합 전처리 파이프라인\n",
            "================================================================================\n",
            "\n",
            "원본 텍스트:\n",
            "  \"2024년12월1일에 010-1234-5678로 연락주세요. 가격은 12,000원입니다.\"\n",
            "\n",
            "최종 토큰:\n",
            "  ['date에', 'phone로', '연락주세요.', '가격은', 'num원입니다.']\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"[예제 5] 통합 전처리 파이프라인\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Preprocessor 초기화\n",
        "preprocessor = TextPreprocessor(\n",
        "    stopwords=KOREAN_STOPWORDS,\n",
        "    remove_stopwords_flag=True,\n",
        "    normalize_flag=True,\n",
        "    regularize_flag=True\n",
        ")\n",
        "\n",
        "# 테스트 텍스트\n",
        "test_text = \"2024년12월1일에 010-1234-5678로 연락주세요. 가격은 12,000원입니다.\"\n",
        "\n",
        "print(\"\\n원본 텍스트:\")\n",
        "print(\"  \\\"%s\\\"\" % test_text)\n",
        "\n",
        "# 전처리 실행\n",
        "tokens = preprocessor.preprocess(test_text)\n",
        "\n",
        "print(\"\\n최종 토큰:\")\n",
        "print(\"  %s\" % tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0676c524",
      "metadata": {
        "id": "0676c524"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d29d59bf",
      "metadata": {
        "id": "d29d59bf"
      },
      "source": [
        "## 6. 의료 판독문 전처리 실습"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "50964638",
      "metadata": {
        "id": "50964638"
      },
      "source": [
        "### 의료 도메인 특화 불용어"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9f56ccef",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9f56ccef",
        "outputId": "57bf709c-4f7e-4637-e8fd-43b5467e4798"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "의료 도메인 불용어 통계\n",
            "================================================================================\n",
            "\n",
            "일반 한국어 불용어: 62개\n",
            "의료 도메인 불용어: 86개\n",
            "추가된 불용어: 24개\n"
          ]
        }
      ],
      "source": [
        "# 의료 도메인 불용어 확장\n",
        "MEDICAL_STOPWORDS = KOREAN_STOPWORDS.union(set([\n",
        "    # 의료 불용어\n",
        "    '소견', '상', '님', '환자', '분', '씨',\n",
        "    '있습니다', '없습니다', '보입니다', '됩니다',\n",
        "    '합니다', '하십시오', '하세요',\n",
        "\n",
        "    # 위치 표현\n",
        "    '상부', '하부', '중앙', '전체', '부분', '영역',\n",
        "\n",
        "    # 정도 표현\n",
        "    '경미한', '중등도', '심한', '약간', '다소', '상당한'\n",
        "]))\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"의료 도메인 불용어 통계\")\n",
        "print(\"=\" * 80)\n",
        "print(\"\\n일반 한국어 불용어: %d개\" % len(KOREAN_STOPWORDS))\n",
        "print(\"의료 도메인 불용어: %d개\" % len(MEDICAL_STOPWORDS))\n",
        "print(\"추가된 불용어: %d개\" % (len(MEDICAL_STOPWORDS) - len(KOREAN_STOPWORDS)))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fda875ac",
      "metadata": {
        "id": "fda875ac"
      },
      "source": [
        "### 예제 6: 의료 판독문 전처리"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa725c5f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aa725c5f",
        "outputId": "d2209914-ce14-4a9e-b4e3-bc8c4c65e55d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "[예제 6] 의료 판독문 전처리\n",
            "================================================================================\n",
            "\n",
            "[판독문 1]\n",
            "원본:\n",
            "  \"양측 폐야는 깨끗합니다. 심비대 소견은 없습니다.\"\n",
            "정규화:\n",
            "  \"양측 폐야는 깨끗합니다. 심비대 소견은 없습니다.\"\n",
            "표준화:\n",
            "  \"양측 폐야는 깨끗합니다. 심비대 소견은 없습니다.\"\n",
            "토큰화:\n",
            "  ['양측', '폐야는', '깨끗합니다.', '심비대', '소견은', '없습니다.']\n",
            "최종 (불용어 제거):\n",
            "  ['양측', '폐야는', '깨끗합니다.', '심비대', '소견은', '없습니다.']\n",
            "감소율: 0.0% (6 → 6 토큰)\n",
            "\n",
            "[판독문 2]\n",
            "원본:\n",
            "  \"우상엽에 3.2cm 크기의 결절이 관찰됩니다.\"\n",
            "정규화:\n",
            "  \"우상엽에 NUMcm 크기의 결절이 관찰됩니다.\"\n",
            "표준화:\n",
            "  \"우상엽에 numcm 크기의 결절이 관찰됩니다.\"\n",
            "토큰화:\n",
            "  ['우상엽에', 'numcm', '크기의', '결절이', '관찰됩니다.']\n",
            "최종 (불용어 제거):\n",
            "  ['우상엽에', 'numcm', '크기의', '결절이', '관찰됩니다.']\n",
            "감소율: 0.0% (5 → 5 토큰)\n",
            "\n",
            "[판독문 3]\n",
            "원본:\n",
            "  \"2024-11-15 CT 검사 결과: 간에 1.5cm 음영 보임\"\n",
            "정규화:\n",
            "  \"DATE CT 검사 결과: 간에 NUMcm 음영 보임\"\n",
            "표준화:\n",
            "  \"date ct 검사 결과: 간에 numcm 음영 보임\"\n",
            "토큰화:\n",
            "  ['date', 'ct', '검사', '결과:', '간에', 'numcm', '음영', '보임']\n",
            "최종 (불용어 제거):\n",
            "  ['date', 'ct', '검사', '결과:', '간에', 'numcm', '음영', '보임']\n",
            "감소율: 0.0% (8 → 8 토큰)\n",
            "\n",
            "[판독문 4]\n",
            "원본:\n",
            "  \"환자분은 010-1234-5678로 연락 주세요. 재검사 날짜는 12월 20일입니다.\"\n",
            "정규화:\n",
            "  \"환자분은 PHONE로 연락 주세요. 재검사 날짜는 NUM월 NUM일입니다.\"\n",
            "표준화:\n",
            "  \"환자분은 phone로 연락 주세요. 재검사 날짜는 num월 num일입니다.\"\n",
            "토큰화:\n",
            "  ['환자분은', 'phone로', '연락', '주세요.', '재검사', '날짜는', 'num월', 'num일입니다.']\n",
            "최종 (불용어 제거):\n",
            "  ['환자분은', 'phone로', '연락', '주세요.', '재검사', '날짜는', 'num월', 'num일입니다.']\n",
            "감소율: 0.0% (8 → 8 토큰)\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"[예제 6] 의료 판독문 전처리\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# 의료 전용 Preprocessor\n",
        "medical_preprocessor = TextPreprocessor(\n",
        "    stopwords=MEDICAL_STOPWORDS,\n",
        "    remove_stopwords_flag=True,\n",
        "    normalize_flag=True,\n",
        "    regularize_flag=True\n",
        ")\n",
        "\n",
        "# 샘플 판독문\n",
        "medical_reports = [\n",
        "    \"양측 폐야는 깨끗합니다. 심비대 소견은 없습니다.\",\n",
        "    \"우상엽에 3.2cm 크기의 결절이 관찰됩니다.\",\n",
        "    \"2024-11-15 CT 검사 결과: 간에 1.5cm 음영 보임\",\n",
        "    \"환자분은 010-1234-5678로 연락 주세요. 재검사 날짜는 12월 20일입니다.\"\n",
        "]\n",
        "\n",
        "for i, report in enumerate(medical_reports, 1):\n",
        "    print(\"\\n[판독문 %d]\" % i)\n",
        "\n",
        "    results = medical_preprocessor.preprocess_full(report)\n",
        "\n",
        "    print(\"원본:\")\n",
        "    print(\"  \\\"%s\\\"\" % results['original'])\n",
        "\n",
        "    print(\"정규화:\")\n",
        "    print(\"  \\\"%s\\\"\" % results['regularized'])\n",
        "\n",
        "    print(\"표준화:\")\n",
        "    print(\"  \\\"%s\\\"\" % results['normalized'])\n",
        "\n",
        "    print(\"토큰화:\")\n",
        "    print(\"  %s\" % results['tokenized'])\n",
        "\n",
        "    print(\"최종 (불용어 제거):\")\n",
        "    print(\"  %s\" % results['filtered'])\n",
        "\n",
        "    print(\"감소율: %.1f%% (%d → %d 토큰)\" % (\n",
        "        100.0 * (len(results['tokenized']) - len(results['filtered'])) / len(results['tokenized']),\n",
        "        len(results['tokenized']),\n",
        "        len(results['filtered'])\n",
        "    ))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "TEIC65lhBk1e"
      },
      "id": "TEIC65lhBk1e"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2b47f804"
      },
      "source": [
        "## 7. Summary\n",
        "\n",
        "본 핸즈온 실습에서는 텍스트 전처리 파이프라인 구축을 목표로 다음과 같은 주요 기법들을 학습하고 구현했습니다.\n",
        "\n",
        "1.  **불용어(Stopwords) 제거**: 텍스트 분석에서 의미 없는 단어(조사, 어미, 관사 등)를 제거하여 데이터의 노이즈를 줄이고 핵심 의미에 집중할 수 있도록 했습니다. 한국어 및 영어 불용어 리스트를 예시로 `remove_stopwords` 함수를 구현하여 적용했습니다.\n",
        "\n",
        "2.  **표준화(Normalization)**: 동일한 의미를 가진 다양한 텍스트 형태를 통일된 형태로 변환했습니다. 이를 위해 `normalize_text` 함수를 통해 대소문자 통일, 반복 문자/구두점 축약, 다중 공백 처리 등의 기법을 적용했습니다.\n",
        "\n",
        "3.  **정규화(Regularization)**: 특정 패턴(숫자, 날짜, 전화번호 등)을 대표 토큰(NUM, DATE, PHONE)으로 치환하여 Vocabulary 크기를 줄이고 모델의 일반화 성능을 향상시켰습니다. `normalize_numbers`, `normalize_dates`, `normalize_phone_numbers` 함수를 구현하여 순서대로 적용하는 `normalize_patterns`를 만들었습니다.\n",
        "\n",
        "4.  **통합 전처리 파이프라인**: 위에서 구현한 개별 전처리 단계들을 `TextPreprocessor` 클래스 하나로 통합했습니다. 이 클래스는 불용어 제거, 표준화, 정규화 적용 여부를 선택적으로 설정할 수 있도록 설계되었으며, `preprocess` 및 `preprocess_full` 메서드를 제공합니다.\n",
        "\n",
        "5.  **의료 판독문 전처리 실습**: 마지막으로 실제 의료 도메인의 특성을 반영하여 일반 한국어 불용어 리스트를 확장한 `MEDICAL_STOPWORDS`를 정의하고, `TextPreprocessor`를 활용하여 샘플 의료 판독문을 전처리하는 과정을 실습했습니다. 이 과정을 통해 각 전처리 단계가 텍스트에 어떻게 적용되고 최종 결과에 어떤 영향을 미치는지 확인했습니다.\n",
        "\n",
        "이 실습을 통해 텍스트 데이터의 품질을 향상시키고, 이후 텍스트 분석 및 자연어 처리 모델링에 적합한 형태로 데이터를 가공하는 핵심 역량을 습득할 수 있었습니다."
      ],
      "id": "2b47f804"
    },
    {
      "cell_type": "markdown",
      "id": "f14e7057",
      "metadata": {
        "id": "f14e7057"
      },
      "source": [
        "---"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}