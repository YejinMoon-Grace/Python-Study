{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c49575a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.20.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.5 kB)\n",
      "Collecting absl-py>=1.0.0 (from tensorflow)\n",
      "  Using cached absl_py-2.3.1-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow)\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting flatbuffers>=24.3.25 (from tensorflow)\n",
      "  Using cached flatbuffers-25.9.23-py2.py3-none-any.whl.metadata (875 bytes)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow)\n",
      "  Downloading gast-0.7.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting google_pasta>=0.1.1 (from tensorflow)\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
      "Collecting libclang>=13.0.0 (from tensorflow)\n",
      "  Using cached libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl.metadata (5.2 kB)\n",
      "Collecting opt_einsum>=2.3.2 (from tensorflow)\n",
      "  Using cached opt_einsum-3.4.0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: packaging in /home/kdt_43/miniconda3/lib/python3.9/site-packages (from tensorflow) (25.0)\n",
      "Requirement already satisfied: protobuf>=5.28.0 in /home/kdt_43/miniconda3/lib/python3.9/site-packages (from tensorflow) (6.33.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/kdt_43/miniconda3/lib/python3.9/site-packages (from tensorflow) (2.32.5)\n",
      "Requirement already satisfied: setuptools in /home/kdt_43/miniconda3/lib/python3.9/site-packages (from tensorflow) (80.9.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /home/kdt_43/miniconda3/lib/python3.9/site-packages (from tensorflow) (1.17.0)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow)\n",
      "  Downloading termcolor-3.1.0-py3-none-any.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: typing_extensions>=3.6.6 in /home/kdt_43/miniconda3/lib/python3.9/site-packages (from tensorflow) (4.15.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /home/kdt_43/miniconda3/lib/python3.9/site-packages (from tensorflow) (2.0.1)\n",
      "Collecting grpcio<2.0,>=1.24.3 (from tensorflow)\n",
      "  Downloading grpcio-1.76.0-cp39-cp39-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (3.7 kB)\n",
      "Collecting tensorboard~=2.20.0 (from tensorflow)\n",
      "  Downloading tensorboard-2.20.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting keras>=3.10.0 (from tensorflow)\n",
      "  Downloading keras-3.10.0-py3-none-any.whl.metadata (6.0 kB)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /home/kdt_43/miniconda3/lib/python3.9/site-packages (from tensorflow) (2.0.2)\n",
      "Collecting h5py>=3.11.0 (from tensorflow)\n",
      "  Downloading h5py-3.14.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.7 kB)\n",
      "Collecting ml_dtypes<1.0.0,>=0.5.1 (from tensorflow)\n",
      "  Downloading ml_dtypes-0.5.4-cp39-cp39-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (8.9 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/kdt_43/miniconda3/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorflow) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/kdt_43/miniconda3/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorflow) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/kdt_43/miniconda3/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorflow) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/kdt_43/miniconda3/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorflow) (2025.10.5)\n",
      "Collecting markdown>=2.6.8 (from tensorboard~=2.20.0->tensorflow)\n",
      "  Downloading markdown-3.9-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: pillow in /home/kdt_43/miniconda3/lib/python3.9/site-packages (from tensorboard~=2.20.0->tensorflow) (11.3.0)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard~=2.20.0->tensorflow)\n",
      "  Using cached tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /home/kdt_43/miniconda3/lib/python3.9/site-packages (from tensorboard~=2.20.0->tensorflow) (3.1.4)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /home/kdt_43/miniconda3/lib/python3.9/site-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
      "Requirement already satisfied: rich in /home/kdt_43/miniconda3/lib/python3.9/site-packages (from keras>=3.10.0->tensorflow) (14.2.0)\n",
      "Collecting namex (from keras>=3.10.0->tensorflow)\n",
      "  Using cached namex-0.1.0-py3-none-any.whl.metadata (322 bytes)\n",
      "Collecting optree (from keras>=3.10.0->tensorflow)\n",
      "  Downloading optree-0.18.0-cp39-cp39-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (34 kB)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /home/kdt_43/miniconda3/lib/python3.9/site-packages (from markdown>=2.6.8->tensorboard~=2.20.0->tensorflow) (8.5.0)\n",
      "Requirement already satisfied: zipp>=3.20 in /home/kdt_43/miniconda3/lib/python3.9/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard~=2.20.0->tensorflow) (3.21.0)\n",
      "Requirement already satisfied: markupsafe>=2.1.1 in /home/kdt_43/miniconda3/lib/python3.9/site-packages (from werkzeug>=1.0.1->tensorboard~=2.20.0->tensorflow) (3.0.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/kdt_43/miniconda3/lib/python3.9/site-packages (from rich->keras>=3.10.0->tensorflow) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/kdt_43/miniconda3/lib/python3.9/site-packages (from rich->keras>=3.10.0->tensorflow) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/kdt_43/miniconda3/lib/python3.9/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.10.0->tensorflow) (0.1.2)\n",
      "Downloading tensorflow-2.20.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (620.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m620.4/620.4 MB\u001b[0m \u001b[31m28.6 MB/s\u001b[0m  \u001b[33m0:00:18\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading grpcio-1.76.0-cp39-cp39-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (6.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m53.5 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading ml_dtypes-0.5.4-cp39-cp39-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (5.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.0/5.0 MB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tensorboard-2.20.0-py3-none-any.whl (5.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m45.0 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n",
      "Using cached absl_py-2.3.1-py3-none-any.whl (135 kB)\n",
      "Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Using cached flatbuffers-25.9.23-py2.py3-none-any.whl (30 kB)\n",
      "Downloading gast-0.7.0-py3-none-any.whl (22 kB)\n",
      "Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Downloading h5py-3.14.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m31.2 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading keras-3.10.0-py3-none-any.whl (1.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m27.8 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl (24.5 MB)\n",
      "Downloading markdown-3.9-py3-none-any.whl (107 kB)\n",
      "Using cached opt_einsum-3.4.0-py3-none-any.whl (71 kB)\n",
      "Downloading termcolor-3.1.0-py3-none-any.whl (7.7 kB)\n",
      "Using cached namex-0.1.0-py3-none-any.whl (5.9 kB)\n",
      "Downloading optree-0.18.0-cp39-cp39-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (386 kB)\n",
      "Installing collected packages: namex, libclang, flatbuffers, termcolor, tensorboard-data-server, optree, opt_einsum, ml_dtypes, h5py, grpcio, google_pasta, gast, astunparse, absl-py, markdown, tensorboard, keras, tensorflow\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18/18\u001b[0m [tensorflow]8\u001b[0m [tensorflow]]data-server]\n",
      "\u001b[1A\u001b[2KSuccessfully installed absl-py-2.3.1 astunparse-1.6.3 flatbuffers-25.9.23 gast-0.7.0 google_pasta-0.2.0 grpcio-1.76.0 h5py-3.14.0 keras-3.10.0 libclang-18.1.1 markdown-3.9 ml_dtypes-0.5.4 namex-0.1.0 opt_einsum-3.4.0 optree-0.18.0 tensorboard-2.20.0 tensorboard-data-server-0.7.2 tensorflow-2.20.0 termcolor-3.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37b433a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting config\n",
      "  Downloading config-0.5.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Downloading config-0.5.1-py2.py3-none-any.whl (20 kB)\n",
      "Installing collected packages: config\n",
      "Successfully installed config-0.5.1\n"
     ]
    }
   ],
   "source": [
    "!pip install config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f58269",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting utils\n",
      "  Downloading utils-1.0.2.tar.gz (13 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: utils\n",
      "  Building wheel for utils (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for utils: filename=utils-1.0.2-py2.py3-none-any.whl size=13983 sha256=eb5abb988b4a3930136736b23c5fbb138494376aed4a3b90e74e21119d11ac86\n",
      "  Stored in directory: /home/kdt_43/.cache/pip/wheels/4c/a5/a3/ab48e06c936b39960801612ee2767ff53764119f33d3d646e7\n",
      "Successfully built utils\n",
      "Installing collected packages: utils\n",
      "Successfully installed utils-1.0.2\n"
     ]
    }
   ],
   "source": [
    "!pip install utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e9cf470",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-18 11:36:50.993800: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-12-18 11:36:51.008374: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-12-18 11:36:51.629189: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-12-18 11:36:54.706905: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-12-18 11:36:54.710724: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "class utils:\n",
    "    def __init__(self, input_size):\n",
    "        self.data_size = input_size\n",
    "        self.alpha = 0.1\n",
    "        self.initializer = tf.contrib.layers.variance_scaling_initializer()  # HE initializer.\n",
    "        self.regularizer = None  # tf.contrib.layers.l2_regularizer(0.00001)\n",
    "\n",
    "\n",
    "    def init_weight_bias(self, name, shape, filtercnt, trainable):\n",
    "        weights = tf.get_variable(name=name + \"w\", shape=shape,\n",
    "                                  initializer=tf.contrib.layers.xavier_initializer_conv2d(),\n",
    "                                  dtype=tf.float32, trainable=trainable)\n",
    "        biases = tf.Variable(initial_value=tf.constant(0.1, shape=[filtercnt], dtype=tf.float32), name=name + \"b\",\n",
    "                             trainable=trainable)\n",
    "        return weights, biases\n",
    "\n",
    "    def init_weight_bias_3d(self, name, shape, filtercnt, trainable):\n",
    "        weights = tf.get_variable(name=name + \"w\", shape=shape,\n",
    "                                  initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                  dtype=tf.float32, trainable=trainable)\n",
    "        biases = tf.Variable(initial_value=tf.constant(0.1, shape=[filtercnt], dtype=tf.float32), name=name + \"b\",\n",
    "                             trainable=trainable)\n",
    "        return weights, biases\n",
    "    def layer_conv2D(self, name, inputs, filters, kernel_size, strides, padding='valid'):\n",
    "        \"\"\"\n",
    "        2D convolution using tf.layers\n",
    "        There are different 2D convolution functions at tensorflow like tf.nn.conv2d, tf.layers.conv2d, tf.contrib.layers.conv2d.\n",
    "        Args:\n",
    "            name: Layer name. string type.\n",
    "            inputs: Input data. tensor type.\n",
    "            filters: number of output channel. int type.\n",
    "            kernel_size: list of filter size. list or tuple type. ex) if you want to use 3x3 filter, filters should be [3, 3]\n",
    "            strides: number of stride. int type.\n",
    "            padding: string of padding option. Options are 'valid' and 'same'. string type.\n",
    "\n",
    "        Returns: convolution results, tensor type.\n",
    "\n",
    "        \"\"\"\n",
    "        conv2D = tf.layers.conv2d(inputs=inputs, filters=filters,\n",
    "                                  kernel_size=kernel_size, strides=strides,\n",
    "                                  padding=padding, use_bias=True,\n",
    "                                  kernel_initializer=self.initializer,\n",
    "                                  kernel_regularizer=self.regularizer,\n",
    "                                  name=name)\n",
    "        return conv2D\n",
    "\n",
    "    def layer_conv3D(self, name, inputs, filters, kernel_size, strides, padding='valid'):\n",
    "        conv3D = tf.layers.conv3d(inputs=inputs, filters=filters, kernel_size=kernel_size, strides=strides,\n",
    "                                  padding=padding, use_bias=True, kernel_initializer=self.initializer,\n",
    "                                  kernel_regularizer=self.regularizer, name=name)\n",
    "        return conv3D\n",
    "\n",
    "    def layer_s_conv2D(self,name, inputs, filters, kernel_size, strides, padding='valid'):\n",
    "        \"\"\"\n",
    "        2D seperable convolution using tf.layers\n",
    "        Also, there are different 2D separable convolution at tensorflow.\n",
    "        Args:\n",
    "            name: Layer name. string type.\n",
    "            inputs: Input data. tensor type.\n",
    "            filters: number of output channel. int type.\n",
    "            kernel_size: list of filter size. list or tuple type. ex) if you want to use 3x3 filter, filters should be [3, 3]\n",
    "            strides: number of stride. int type.\n",
    "            padding: string of padding option. Options are 'valid' and 'same'. string type.\n",
    "\n",
    "        Returns: seperable convolution results, tensor type.\n",
    "\n",
    "        \"\"\"\n",
    "        s_conv2D = tf.layers.separable_conv2d(inputs=inputs, filters=filters, kernel_size=kernel_size, strides=strides,\n",
    "                                              padding=padding, use_bias=True,\n",
    "                                              depthwise_initializer=self.initializer,\n",
    "                                              depthwise_regularizer=self.regularizer,\n",
    "                                              pointwise_initializer=self.initializer,\n",
    "                                              pointwise_regularizer=self.regularizer,\n",
    "                                              name=name)\n",
    "        return s_conv2D\n",
    "\n",
    "    def conv_layer(self, data, weight, bias, padding):\n",
    "        conv = tf.nn.conv2d(input=data, filter=weight, strides=[1, 1, 1, 1], padding=padding)\n",
    "        # return tf.nn.relu(tf.nn.bias_add(conv, bias))\n",
    "        return tf.nn.bias_add(conv, bias)\n",
    "\n",
    "    def squeeze_layer(self, output):\n",
    "        return tf.squeeze(output, squeeze_dims=-1)\n",
    "\n",
    "    def up_conv_layer(self, data, weight, bias, padding):\n",
    "\n",
    "        shape_output = data.get_shape().as_list()\n",
    "        shape_output[1] *= 2\n",
    "        shape_output[2] *= 2\n",
    "        shape_output[-1] = shape_output[-1] // 2\n",
    "\n",
    "        upconv = tf.nn.conv2d_transpose(value=data, filter=weight, output_shape=shape_output,\n",
    "                                        strides=[1, 2, 2, 1], padding=padding)\n",
    "\n",
    "        return tf.nn.relu(tf.nn.bias_add(upconv, bias))\n",
    "\n",
    "    def up_conv_3d_layer(self, data, weight, bias, stride, padding):\n",
    "\n",
    "        shape_output = data.get_shape().as_list()\n",
    "        shape_output[1] *= 2\n",
    "        shape_output[2] *= 2\n",
    "        shape_output[3] *= 2\n",
    "        shape_output[-1] = bias.get_shape().as_list()[0]\n",
    "\n",
    "        upconv = tf.nn.conv3d_transpose(value=data, filter=weight, output_shape=shape_output,\n",
    "                                        strides=stride, padding=padding)\n",
    "\n",
    "        return tf.nn.bias_add(upconv, bias)\n",
    "\n",
    "\n",
    "    def crop_concat(self, data, conv):\n",
    "\n",
    "        # crop and concat (x1_shape[1] - x2_shape[1]) // 2, (x1_shape[2] - x2_shape[2])\n",
    "        shape1 = conv.get_shape().as_list()\n",
    "        shape2 = data.get_shape().as_list()\n",
    "        offsets = [0, (shape1[1]-shape2[1]) // 2, (shape1[2]-shape2[2]) // 2, 0]\n",
    "        size = [-1, shape2[1], shape2[2], -1]\n",
    "        crop_conv = tf.slice(conv, offsets, size)\n",
    "\n",
    "        return tf.concat([data, crop_conv], axis=3)\n",
    "\n",
    "    def crop_concat_3d(self, data, conv):\n",
    "\n",
    "        # crop and concat (x1_shape[1] - x2_shape[1]) // 2, (x1_shape[2] - x2_shape[2])\n",
    "        shape1 = conv.get_shape().as_list()\n",
    "        shape2 = data.get_shape().as_list()\n",
    "        offsets = [0, (shape1[1]-shape2[1]) // 2, (shape1[2]-shape2[2]) // 2, (shape1[3]-shape2[3]) // 2, 0]\n",
    "        size = [-1, shape2[1], shape2[2], shape2[3], -1]\n",
    "        crop_conv = tf.slice(conv, offsets, size)\n",
    "\n",
    "        return tf.concat([data, crop_conv], axis=-1)\n",
    "\n",
    "    def reshape_layer(self, conv):\n",
    "        return tf.expand_dims(conv, -1)\n",
    "\n",
    "    def conv3d_layer(self, data, weight, bias, stride, padding):\n",
    "        conv = tf.nn.conv3d(input=data, filter=weight, strides=stride, padding=padding)\n",
    "        return tf.nn.bias_add(conv, bias)\n",
    "\n",
    "    def batch_norm_layer(self, data, train=True):\n",
    "        return tf.contrib.layers.batch_norm(inputs=data, is_training=train)\n",
    "\n",
    "    def parametric_relu_layer(self, conv, alpha):\n",
    "        return alpha * conv if conv < 0 else tf.nn.relu(conv)\n",
    "\n",
    "    def relu_layer(self, conv):\n",
    "        return tf.nn.relu(conv)\n",
    "\n",
    "    def leaky_relu_layer(self, conv, alpha):\n",
    "        return tf.nn.relu(conv) - alpha * tf.nn.relu(-conv)\n",
    "\n",
    "    def depth_wise_conv_layer(self, data, weight, bias, padding, is_inception):\n",
    "        conv = tf.nn.depthwise_conv2d(input=data, filter=weight, strides=[1, 1, 1, 1], padding=padding)\n",
    "        if is_inception:\n",
    "            return tf.nn.bias_add(conv, bias)\n",
    "        return tf.nn.relu(tf.nn.bias_add(conv, bias))\n",
    "\n",
    "\n",
    "    def deconv2D(self, name, inputs, filter_shape, output_shape, strides, padding='valid'):\n",
    "        \"\"\"\n",
    "        2D transpose convolution using tf.nn\n",
    "        Also, there are different 2D transpose convolution at tensorflow.\n",
    "        Some bugs are occured at tf.layers.conv2d_transpose, so manually using tf.nn.conv2d_tranpose.\n",
    "        Weight(W) and shape, batchsize, output shape must be declared manually.\n",
    "        Args:\n",
    "            name: Layer name. string type.\n",
    "            inputs: Input data. tensor type.\n",
    "            filter_shape: list of filter shape, shape will be [filter size, filter size, output channel, input channel]. list of tuple type.\n",
    "            output_shape: list of output shape, shape will be [batch_size, output size, output size, output channel], -1 for automatically fitting to batch size. list or tuple type.\n",
    "            strides: list of stride shape, if you want to stride [2, 2], shape will be [1, 2, 2, 1]\n",
    "            padding: string of padding option. Options are 'valid' and 'same'. string type.\n",
    "\n",
    "        Returns: transpose convolution results, tensor type.\n",
    "        \"\"\"\n",
    "\n",
    "        W = tf.get_variable(name + 'W', filter_shape, initializer=self.initializer, regularizer=self.regularizer)\n",
    "        # shape = tf.shape(inputs)\n",
    "\n",
    "        # output_shape2 = [batch_size, output_shape[1], output_shape[2], output_shape[3]]\n",
    "        layer = tf.nn.conv2d_transpose(inputs, filter=W, output_shape=output_shape, strides=strides, padding=padding)\n",
    "        return layer\n",
    "\n",
    "    def re_conv2D(self, name, inputs, output_shape):\n",
    "        \"\"\"\n",
    "        https://distill.pub/2016/deconv-checkerboard/\n",
    "        re-convolution can replace transpose convolution. Used at Cycle GAN.\n",
    "        Instead of transpose convolution, resize images with nearest neighbor interpolation, after then using 1x1 convolution for reshaping channels.\n",
    "        Args:\n",
    "            name: Layer name. string type.\n",
    "            inputs: Input data. tensor type.\n",
    "            output_shape: list of output shape, shape will be [-1, output size, output size, output channel], -1 for automatically fitting to batch size. list or tuple type.\n",
    "\n",
    "        Returns: resize convolution results, tensor type.\n",
    "\n",
    "        \"\"\"\n",
    "        resize_layer = tf.image.resize_nearest_neighbor(images=inputs, size=[output_shape[1], output_shape[2]],\n",
    "                                                        name=name + '_resizing')\n",
    "        # padding_layer = tf.pad(resize_layer)\n",
    "        # conv_layer = conv2D(padding_layer)\n",
    "        conv_layer = self.layer_conv2D(name=name + '_conv', inputs=resize_layer, filters=output_shape[3],\n",
    "                                       kernel_size=[3, 3], strides=[1, 1], padding='same')\n",
    "        return conv_layer\n",
    "\n",
    "    def pool_layer(self, data, kernel=[1, 2, 2, 1], stride=[1, 2, 2, 1], padding='VALID'):\n",
    "        # kernel = [1, 2, 2, 1] stride = [1, 2, 2, 1]\n",
    "        return tf.nn.max_pool(value=data, ksize=kernel, strides=stride, padding=padding)\n",
    "\n",
    "    def dense_layer(self, data, unit, drop, trainable):\n",
    "        shape = data.get_shape().as_list()\n",
    "        data_flat = tf.reshape(data, [shape[0], np.prod(shape[1:])])\n",
    "        dropout = tf.layers.dropout(inputs=data_flat, rate=drop, training=trainable)\n",
    "        dense = tf.layers.dense(inputs=dropout, units=unit, activation=tf.nn.relu)\n",
    "        dropout = tf.layers.dropout(inputs=dense, rate=drop, training=trainable)\n",
    "        dense = tf.layers.dense(inputs=dropout, units=np.prod(shape[1:]), activation=tf.nn.relu)\n",
    "\n",
    "        return tf.reshape(dense, [shape[0], shape[1], shape[2], shape[3]])\n",
    "\n",
    "    def fc_layer(self, data, dropout, trainable):\n",
    "        shape = data.get_shape().as_list()\n",
    "        shape = [np.prod(shape[1:]), shape[0]]\n",
    "\n",
    "        w, b = self.init_weight_bias(name='fc', shape=shape, filtercnt=shape[-1], trainable=trainable)\n",
    "\n",
    "        hidden = tf.nn.bias_add(tf.matmul(tf.reshape(data, [shape[1], shape[0]]), w), b)\n",
    "        hidden = tf.nn.relu(hidden)\n",
    "        if dropout < 1.:\n",
    "            hidden = tf.nn.dropout(hidden, dropout)\n",
    "        return hidden\n",
    "\n",
    "    def fc_layer_weight(self, data, weight, bias, dropout):\n",
    "        shape = data.get_shape().as_list()\n",
    "        shape = [shape[0], np.prod(shape[1:])]\n",
    "\n",
    "        hidden = tf.nn.bias_add(tf.matmul(tf.reshape(data, shape), weight), bias)\n",
    "\n",
    "        hidden = tf.nn.relu(hidden)\n",
    "        if dropout < 1.:\n",
    "            hidden = tf.nn.dropout(hidden, dropout)\n",
    "        return hidden\n",
    "\n",
    "    def fc_concat_layer(self, data1, data2, weight, bias, dropout, batch_norm=False):\n",
    "        data = tf.concat([data1, data2], axis=3)\n",
    "        shape = data.get_shape().as_list()\n",
    "        shape = [shape[0], np.prod(shape[1:])]\n",
    "\n",
    "        hidden = tf.nn.bias_add(tf.matmul(tf.reshape(data, shape), weight), bias)\n",
    "        if batch_norm:\n",
    "            hidden = tf.contrib.layers.batch_norm(inputs=hidden, is_training=True)\n",
    "            # self.batch_norm_layer(hidden)\n",
    "        hidden = tf.nn.relu(hidden)\n",
    "        if dropout < 1.:\n",
    "            hidden = tf.nn.dropout(hidden, dropout)\n",
    "        return hidden\n",
    "\n",
    "    def huber_loss(self, labels, predictions, delta=1.0):\n",
    "        residual = tf.abs(predictions - labels)\n",
    "        condition = tf.less(residual, delta)\n",
    "        small_res = 0.5 * tf.square(residual)\n",
    "        large_res = delta * residual - 0.5 * tf.square(delta)\n",
    "        return tf.where(condition, small_res, large_res)\n",
    "\n",
    "    def conv_out_size_same(self, size, stride):\n",
    "            return int(math.ceil(float(size) / float(stride)))\n",
    "\n",
    "    def modified_focal_loss(self, output, target, total_weight=1., b_weight=0.5):\n",
    "        \"\"\"\n",
    "        focal loss is loss function for detect rare, small class. This is modified focal loss, modified by LYE.\n",
    "        Args:\n",
    "            output: predicted, output of network\n",
    "            target: groundtruth, label data\n",
    "            total_weight: total weight\n",
    "            b_weight: background weight\n",
    "\n",
    "        Returns: loss value.\n",
    "\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        기존에 사용하던 focal_loss는 foreground area(object label이 있는 area)의 오차만을 loss 값에 반영하였는데, \n",
    "        이렇게 하니 background area(object label이 없는 area)에서 마구잡이로 object가 있다고 찾아버리는 문제가 발생한다.\n",
    "        이를 해결하기 위해 background area의 예측 오차도 loss 값에 반영하도록 하였다.\n",
    "\n",
    "        :param output: 모델을 통해 나온 예측값\n",
    "        :param target: 정답 라벨\n",
    "        :param total_weight: 총 loss 값의 크기를 조절하는 가중치.\n",
    "        :param b_weight: background loss의 반영 비율을 결정하는 가중치.\n",
    "                가중치를 낮게 줄수록 foreground area를 민감하게 잡을 수 있고, 0으로 하면 foreground loss만 100% 반영할 수 있다.\n",
    "        :return: 산출된 loss 값. total_weight * (foreground loss + (b_weight * background loss))\n",
    "        \"\"\"\n",
    "        foreground_predicted, background_predicted = tf.split(output, [1, 1], 3)\n",
    "        foreground_truth, background_truth = tf.split(target, [1, 1], 3)\n",
    "\n",
    "        foreground_loss = self.focal_loss(output=foreground_predicted, target=foreground_truth)\n",
    "        background_loss = self.focal_loss(output=background_predicted, target=background_truth)\n",
    "\n",
    "        return total_weight * (((1 - b_weight) * foreground_loss) + (b_weight * background_loss))\n",
    "\n",
    "    def focal_loss(self, output, target, smooth=1e-6):\n",
    "        \"\"\"\n",
    "        Original focal loss. focal loss is loss function for detect rare, small class.\n",
    "        Check https://arxiv.org/pdf/1708.02002.pdf, focal loss paper and https://arxiv.org/pdf/1711.01506.pdf, focal unet paper.\n",
    "        Args:\n",
    "            output: predicted, output of network\n",
    "            target: groundtruth, label data\n",
    "            smooth: very small value for avoid zero divide.\n",
    "\n",
    "        Returns: loss value.\n",
    "\n",
    "        \"\"\"\n",
    "        output, _ = tf.split(output, [1, 1], 3)\n",
    "        target, _ = tf.split(target, [1, 1], 3)\n",
    "        focal_matrix = -tf.square(tf.ones_like(output) - output) * target * tf.log(output + smooth)\n",
    "        focal = tf.reduce_sum(focal_matrix)\n",
    "        return focal\n",
    "\n",
    "    # def focal_loss_backup(output, target, smooth=1e-6):\n",
    "    #     focal = -tf.reduce_sum(tf.square(tf.ones_like(output) - output) * target * tf.log(output + smooth))\n",
    "    #     return focal\n",
    "\n",
    "    # '''\n",
    "    # 문제 : label이 없는 경우 predict에서 픽셀을 단 하나만 집어도 로스가 매우 크게 적용된다.\n",
    "    # 대안 : inse, l, r의 reduce_sum을 reduce_mean으로 수정\n",
    "    # 1. pixel-wise로 각각 곱해준다\n",
    "    # 2. 배치단위로 각각 평균을 내준다\n",
    "    # 3. 배치별로 dice loss를 구한다\n",
    "    # 4. 배치 전체를 평균낸다\n",
    "    #\n",
    "    # * 추가 대안\n",
    "    # 1. 틀린 픽셀의 갯수에 비례해서 로그적으로 로스가 증가하게 한다\n",
    "    # 2. 있는 걸 없다고 체크한 오답에 대해 더 큰 로스를 적용한다\n",
    "    # '''\n",
    "    def mean_square_loss(self, output, target):\n",
    "        return tf.losses.mean_squared_error(labels=target, predictions=output)\n",
    "\n",
    "    def modified_dice_loss(self, output, target, axis=(1, 2, 3), smooth=1e-6):\n",
    "        \"\"\"\n",
    "        dice loss is loss function using as true positives/(true positives + false negatives + false positives). This is modified dice loss, modified by LYE.\n",
    "        similar with IoU.\n",
    "        Args:\n",
    "            output: predicted, output of network\n",
    "            target: groundtruth, label data\n",
    "            axis: axis for reduce_sum. if 2D dataset like [batch, height, width, channel], axis will be [1, 2, 3] or (1, 2, 3)\n",
    "            smooth: very small value for avoid zero divide.\n",
    "\n",
    "        Returns: loss value\n",
    "\n",
    "        \"\"\"\n",
    "        output, _ = tf.split(output, [1, 1], 3)\n",
    "        target, _ = tf.split(target, [1, 1], 3)\n",
    "\n",
    "        inse = tf.reduce_mean(output * target, axis=axis)\n",
    "        l = tf.reduce_mean(output * output, axis=axis)\n",
    "        r = tf.reduce_mean(target * target, axis=axis)\n",
    "        dice = (2. * inse + smooth) / (l + r + smooth)\n",
    "        dice = tf.reduce_mean(dice)\n",
    "        return 1 - dice\n",
    "\n",
    "    def dice_loss(self, output, target, axis=(1, 2, 3), smooth=1e-6):\n",
    "        \"\"\"\n",
    "        dice loss is loss function using as true positives/(true positives + false negatives + false positives).\n",
    "        similar with IoU.\n",
    "        Args:\n",
    "            output: predicted, output of network\n",
    "            target: groundtruth, label data\n",
    "            axis: axis for reduce_sum. if 2D dataset like [batch, height, width, channel], axis will be [1, 2, 3] or (1, 2, 3)\n",
    "            smooth: very small value for avoid zero divide.\n",
    "\n",
    "        Returns: loss value\n",
    "\n",
    "        \"\"\"\n",
    "        output, _ = tf.split(output, [1, 1], 3)\n",
    "        target, _ = tf.split(target, [1, 1], 3)\n",
    "\n",
    "        inse = tf.reduce_sum(output * target, axis=axis)\n",
    "        l = tf.reduce_sum(output * output, axis=axis)\n",
    "        r = tf.reduce_sum(target * target, axis=axis)\n",
    "        dice = (2. * inse + smooth) / (l + r + smooth)\n",
    "        dice = tf.reduce_mean(dice)\n",
    "        return 1 - dice\n",
    "\n",
    "    def cross_entropy(self, output, target):\n",
    "        return tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=target, logits=output))\n",
    "\n",
    "    def softmax(self, output):\n",
    "        return tf.nn.softmax(output)\n",
    "\n",
    "    def output_layer(self, data, weight, bias, label):\n",
    "        shape = data.get_shape().as_list()\n",
    "        shape = [shape[0], np.prod(shape[1:])]\n",
    "        hidden = tf.nn.bias_add(tf.matmul(tf.reshape(data, shape), weight), bias)\n",
    "\n",
    "        if label is None:\n",
    "            return None, tf.nn.softmax(hidden)\n",
    "        return tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=hidden,\n",
    "                                                                             labels=label)), tf.nn.softmax(hidden)\n",
    "\n",
    "    def pixel_wise_softmax(self, output_map):\n",
    "        with tf.name_scope(\"pixel_wise_softmax\"):\n",
    "            max_axis = tf.reduce_max(output_map, axis=3, keepdims=True)\n",
    "            exponential_map = tf.exp(output_map - max_axis)\n",
    "            normalize = tf.reduce_sum(exponential_map, axis=3, keepdims=True)\n",
    "            return exponential_map / normalize\n",
    "\n",
    "    def pixel_wise_softmax_minsky(self, output_map):\n",
    "        max_axis = tf.reduce_max(output_map, axis=3, keep_dims=True)\n",
    "        exponential_map = tf.exp(output_map - max_axis)\n",
    "        normalize = tf.reduce_sum(exponential_map, axis=3, keep_dims=True)\n",
    "        return exponential_map / normalize\n",
    "\n",
    "    def pixel_wise_cross_entropy(self, label, output_map):\n",
    "        return -tf.reduce_sum(label * tf.log(tf.clip_by_value(output_map, 1e-10, 1.0)), name=\"cross_entropy\")\n",
    "        # logits = tf.log(tf.clip_by_value(output_map, 1e-10, 1.0))\n",
    "        # return tf.nn.sparse_softmax_cross_entropy_with_logits(label, output_map)\n",
    "\n",
    "        # cross_entropy_mean = tf.reduce_mean(cross_entropy, name='xentropy_mean')\n",
    "        # tf.add_to_collection('losses', cross_entropy_mean)\n",
    "\n",
    "        # return tf.add_n(tf.get_collection('losses'), name='total_loss')\n",
    "\n",
    "    def dice_loss2(self, label, logit, smooth=1e-6, num_class=2):\n",
    "        label_oh = tf.squeeze(tf.one_hot(label, num_class, dtype=tf.float32))\n",
    "        logit_sm = tf.squeeze(tf.nn.softmax(logit, axis=-1))\n",
    "\n",
    "        dice_lst = []\n",
    "\n",
    "        for label_ch, logit_ch in zip(tf.split(label_oh, num_class, axis=-1), tf.split(logit_sm, num_class, axis=-1)):\n",
    "            numerator = 2. * tf.reduce_sum(tf.multiply(label_ch, logit_ch))\n",
    "            denominator = tf.reduce_sum(tf.square(label_ch)) + tf.reduce_sum(tf.square(logit_ch))\n",
    "\n",
    "            dice = (numerator + smooth) / (denominator + smooth)\n",
    "            dice_lst.append(dice)\n",
    "\n",
    "        return 1 - tf.reduce_mean(dice_lst)\n",
    "\n",
    "    def pixel_wise_ouput_layer(self, label, data, trainable):\n",
    "\n",
    "        if trainable is False:\n",
    "            return None, self.pixel_wise_softmax(data)\n",
    "        else:\n",
    "            shape1 = label.get_shape().as_list()\n",
    "            shape2 = data.get_shape().as_list()\n",
    "            if not shape1[1:2] == shape2[1:2]:\n",
    "                offsets = [0, (shape1[1] - shape2[1]) // 2, (shape1[2] - shape2[2]) // 2, 0]\n",
    "                size = [-1, shape2[1], shape2[2], -1]\n",
    "                label = tf.slice(label, offsets, size)\n",
    "\n",
    "            # return self.pixel_wise_cross_entropy(label, data), self.pixel_wise_softmax(data)\n",
    "            return self.dice_loss2(label, data, num_class=2), self.pixel_wise_softmax(data)\n",
    "\n",
    "    def input_layer(self, batch_size=128, train=True):\n",
    "        if train:\n",
    "            data_node = tf.placeholder(tf.float32,\n",
    "                                       shape=(batch_size, self.data_size[0], self.data_size[1], self.data_size[2]))\n",
    "            label_node = tf.placeholder(tf.int64, shape=batch_size)\n",
    "        else:\n",
    "            data_node = tf.placeholder(tf.float32,\n",
    "                                       shape=(batch_size, self.data_size[0], self.data_size[1], self.data_size[2]))\n",
    "            label_node = None\n",
    "\n",
    "        return data_node, label_node\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a14dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import config as cfg\n",
    "import ce_an_utils as utils\n",
    "from datetime import datetime\n",
    "import lu_ca_tf_utils as tf_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b7055b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'ce_an_utils'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mce_an_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mutils\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'ce_an_utils'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bba24d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model():\n",
    "    def __init__(self, batch_size):\n",
    "        ## Input\n",
    "        # (batch, depth, height, width, channels)\n",
    "        self.kernel_size = 3\n",
    "        self.b_size = batch_size\n",
    "        # self.X = tf.placeholder(tf.float32, shape=(batch_size, 64, 64, 1))\n",
    "        # self.Y = tf.placeholder(tf.int32, shape=batch_size)\n",
    "        self.X = tf.placeholder(tf.float32, shape=[self.b_size, 64, 64, 1])\n",
    "        self.Y = tf.placeholder(tf.int32, shape=batch_size)\n",
    "\n",
    "        self.train = True\n",
    "\n",
    "        self.utf = tf_utils.utils(input_size=[1, 512, 512, 512, 1])\n",
    "\n",
    "        self.loss, self.logit, self.pred = self.ResidualCNN()\n",
    "\n",
    "    def ResidualCNN(self):\n",
    "\n",
    "        conv_layer_shape = [[self.kernel_size, self.kernel_size, 1, 64],   # conv 64\n",
    "\n",
    "                            [self.kernel_size, self.kernel_size, 64, 64],  # conv\n",
    "                            [self.kernel_size, self.kernel_size, 64, 64],  # conv 64\n",
    "\n",
    "                            [self.kernel_size, self.kernel_size, 64, 64],  # conv\n",
    "                            [self.kernel_size, self.kernel_size, 64, 64],  # conv 64\n",
    "\n",
    "                            [self.kernel_size, self.kernel_size, 64, 64],  # conv\n",
    "                            [self.kernel_size, self.kernel_size, 64, 64],  # conv\n",
    "                            # ---                  pool           ----#\n",
    "                            [self.kernel_size, self.kernel_size, 64, 128],   # conv 32\n",
    "\n",
    "                            [self.kernel_size, self.kernel_size, 128, 128],  # conv\n",
    "                            [self.kernel_size, self.kernel_size, 128, 128],  # conv\n",
    "\n",
    "                            [self.kernel_size, self.kernel_size, 128, 128],  # conv\n",
    "                            [self.kernel_size, self.kernel_size, 128, 128],  # conv\n",
    "\n",
    "                            [self.kernel_size, self.kernel_size, 128, 128],  # conv\n",
    "                            [self.kernel_size, self.kernel_size, 128, 128],  # conv\n",
    "                            # ---                  pool           ----#\n",
    "                            [self.kernel_size, self.kernel_size, 128, 256],  # conv 16\n",
    "\n",
    "                            [self.kernel_size, self.kernel_size, 256, 256],  # conv\n",
    "                            [self.kernel_size, self.kernel_size, 256, 256],  # conv\n",
    "\n",
    "                            [self.kernel_size, self.kernel_size, 256, 256],  # conv\n",
    "                            [self.kernel_size, self.kernel_size, 256, 256],  # conv\n",
    "\n",
    "                            [self.kernel_size, self.kernel_size, 256, 256],  # conv\n",
    "                            [self.kernel_size, self.kernel_size, 256, 256],  # conv\n",
    "                            # ---                  pool           ----#\n",
    "                            [self.kernel_size, self.kernel_size, 256, 512],   # conv 8\n",
    "\n",
    "                            [self.kernel_size, self.kernel_size, 512, 512],   # conv\n",
    "                            [self.kernel_size, self.kernel_size, 512, 512],  # conv\n",
    "\n",
    "                            [self.kernel_size, self.kernel_size, 512, 512],  # conv\n",
    "                            [self.kernel_size, self.kernel_size, 512, 512],  # conv\n",
    "\n",
    "                            [self.kernel_size, self.kernel_size, 512, 512],  # conv\n",
    "                            [self.kernel_size, self.kernel_size, 512, 512],  # conv\n",
    "                            # ---                  pool           ----#\n",
    "                            [self.kernel_size, self.kernel_size, 512, 1024],   # conv 4\n",
    "                            [self.kernel_size, self.kernel_size, 1024, 1024]]  # conv\n",
    "\n",
    "        fc_layer_shape = [[4 * 4 * 1024, 1024], [1024, 1024], [1024, 2]]\n",
    "\n",
    "        batch_size = self.b_size\n",
    "        if self.train:\n",
    "            drop_rate = 0.5\n",
    "        else:\n",
    "            drop_rate = 1.\n",
    "        #     batch_size = self.b_size\n",
    "        train_data_node = self.X\n",
    "        train_labels_node = self.Y #\n",
    "\n",
    "        # else:\n",
    "        #     batch_size = 1\n",
    "        # train_data_node = tf.placeholder(tf.float32, shape=(batch_size, 64, 64, 1))\n",
    "        # train_labels_node = None\n",
    "\n",
    "        cw = []\n",
    "        cb = []\n",
    "\n",
    "        fw = []\n",
    "        fb = []\n",
    "        layers = [train_data_node]\n",
    "\n",
    "        # weight initialization\n",
    "        cross_entropy, softmax = None, None\n",
    "        for kernel, layer_cnt in zip(conv_layer_shape, range(len(conv_layer_shape))):\n",
    "            w, b = self.utf.init_weight_bias(name=\"c%d\" % (layer_cnt), shape=kernel,\n",
    "                                             filtercnt=kernel[-1], trainable=True)\n",
    "            cw.append(w)\n",
    "            cb.append(b)\n",
    "\n",
    "        for kernel, layer_cnt in zip(fc_layer_shape, range(len(fc_layer_shape))):\n",
    "            w, b = self.utf.init_weight_bias(name=\"f%d\" % (layer_cnt), shape=kernel, filtercnt=kernel[-1],\n",
    "                                           trainable=True)\n",
    "            fw.append(w)\n",
    "            fb.append(b)\n",
    "\n",
    "        # connect graph (stack layer)\n",
    "        for w, b, layer_cnt in zip(cw, cb, range(len(cw))):\n",
    "\n",
    "            if layer_cnt == 0 or layer_cnt == 7 or layer_cnt == 14 or layer_cnt == 21 or layer_cnt == 28:\n",
    "                output = self.utf.conv_layer(data=layers[-1], weight=w, bias=b, padding=\"SAME\")\n",
    "                output = self.utf.relu_layer(output)\n",
    "                layers.append(output)\n",
    "                res_node = layers[-1]\n",
    "\n",
    "            elif layer_cnt == 1 or layer_cnt == 8 or layer_cnt == 15 or layer_cnt == 22 or layer_cnt == 29:\n",
    "                output = self.utf.conv_layer(data=layers[-1], weight=w, bias=b, padding=\"SAME\")\n",
    "                output = self.utf.relu_layer(output)\n",
    "                layers.append(output)\n",
    "            elif layer_cnt == 2 or layer_cnt == 9 or layer_cnt == 16 or layer_cnt == 23:\n",
    "                output = self.utf.conv_layer(data=layers[-1], weight=w, bias=b, padding=\"SAME\")\n",
    "                output = tf.add(output, res_node)\n",
    "                output = self.utf.relu_layer(output)\n",
    "                layers.append(output)\n",
    "            elif layer_cnt == 3 or layer_cnt == 10 or layer_cnt == 17 or layer_cnt == 24:\n",
    "                res_node = layers[-1]\n",
    "                output = self.utf.conv_layer(data=layers[-1], weight=w, bias=b, padding=\"SAME\")\n",
    "                output = self.utf.relu_layer(output)\n",
    "                layers.append(output)\n",
    "            elif layer_cnt == 4 or layer_cnt == 11 or layer_cnt == 18 or layer_cnt == 25:\n",
    "                output = self.utf.conv_layer(data=layers[-1], weight=w, bias=b, padding=\"SAME\")\n",
    "                output = tf.add(output, res_node)\n",
    "                output = self.utf.relu_layer(output)\n",
    "                layers.append(output)\n",
    "            elif layer_cnt == 5 or layer_cnt == 12 or layer_cnt == 19 or layer_cnt == 26:\n",
    "                res_node = layers[-1]\n",
    "                output = self.utf.conv_layer(data=layers[-1], weight=w, bias=b, padding=\"SAME\")\n",
    "                output = self.utf.relu_layer(output)\n",
    "                layers.append(output)\n",
    "            elif layer_cnt == 6 or layer_cnt == 13 or layer_cnt == 20 or layer_cnt == 27:\n",
    "                output = self.utf.conv_layer(data=layers[-1], weight=w, bias=b, padding=\"SAME\")\n",
    "                output = tf.add(output, res_node)\n",
    "                output = self.utf.relu_layer(output)\n",
    "                layers.append(output)\n",
    "\n",
    "                output = self.utf.pool_layer(data=layers[-1])\n",
    "                layers.append(output)\n",
    "\n",
    "\n",
    "        for w, b, layer_cnt in zip(fw, fb, range(len(fw))):\n",
    "            if layer_cnt == 2:\n",
    "                cross_entropy, softmax = self.utf.output_layer(data=layers[-1], weight=w, bias=b,\n",
    "                                                                label=train_labels_node)\n",
    "            else:\n",
    "                output1 = self.utf.fc_layer_weight(data=layers[-1], weight=w, bias=b, dropout=drop_rate)\n",
    "                layers.append(output1)\n",
    "\n",
    "        predict = [tf.argmax(tf.cast(softmax > 0.6, tf.float32), 1), tf.argmax(tf.cast(softmax > 0.7, tf.float32), 1), tf.argmax(tf.cast(softmax > 0.8, tf.float32), 1)]\n",
    "\n",
    "        return cross_entropy, softmax, predict"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
